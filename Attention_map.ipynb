{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "714a14ff",
        "outputId": "c8313204-0d9f-4d72-cd6e-b7a246ef780f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TinyViT'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 88 (delta 11), reused 88 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (88/88), 629.01 KiB | 15.34 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/wkcn/TinyViT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Q1pgC8m0_V5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6f7203-2041-45b0-c0f2-8bd0e90ab004"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3f48a5b",
        "outputId": "a51b4d6b-b870-4cb8-ba81-dc269865e5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SparseAttention'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 170 (delta 25), reused 1 (delta 1), pack-reused 125 (from 2)\u001b[K\n",
            "Receiving objects: 100% (170/170), 2.22 MiB | 32.05 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kyegomez/SparseAttention.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffhVRtrS_0nI",
        "outputId": "d6b0913d-3dd3-4bdf-d94f-28c4610799c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'processed-imagenet-dataset-224' dataset.\n",
            "Path to dataset files: /kaggle/input/processed-imagenet-dataset-224\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lyfora/processed-imagenet-dataset-224\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f65aa92",
        "outputId": "6153ab82-4fa3-4568-ff98-75ba70e12094"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Add TinyViT to system path to enable imports\n",
        "if '/content/TinyViT' not in sys.path:\n",
        "    sys.path.append('/content/TinyViT')\n",
        "\n",
        "try:\n",
        "    # Attempt to import the specific model function\n",
        "    from models.tiny_vit import tiny_vit_5m_224\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = tiny_vit_5m_224(pretrained=False)\n",
        "    print(\"Successfully loaded 'tiny_vit_5m_224' model from the TinyViT library.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing model: {e}\")\n",
        "    print(\"Listing TinyViT directory contents for debugging:\")\n",
        "    if os.path.exists('/content/TinyViT'):\n",
        "        print(os.listdir('/content/TinyViT'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/TinyViT/models/tiny_vit.py:640: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.tiny_vit.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/TinyViT/models/tiny_vit.py:653: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.tiny_vit.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/TinyViT/models/tiny_vit.py:666: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.tiny_vit.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/TinyViT/models/tiny_vit.py:679: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.tiny_vit.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/TinyViT/models/tiny_vit.py:693: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.tiny_vit.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 'tiny_vit_5m_224' model from the TinyViT library.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73eeeb6f",
        "outputId": "68501611-db4e-40b2-e9f1-53464217291a"
      },
      "source": [
        "# Install a compatible version of timm\n",
        "!pip install timm==0.9.10"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm==0.9.10 in /usr/local/lib/python3.12/dist-packages (0.9.10)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from timm==0.9.10) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.9.10) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==0.9.10) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from timm==0.9.10) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm==0.9.10) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.9.10) (3.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.10) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.10) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.10) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.9.10) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.9.10) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.9.10) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->timm==0.9.10) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->timm==0.9.10) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.10) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.10) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.10) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.9.10) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcoaMWdrFcD5",
        "outputId": "43878008-b2c4-4ba6-d828-a582b0bac4d1"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "import inspect\n",
        "from models.tiny_vit import TinyViT\n",
        "\n",
        "# Paths\n",
        "config_path = '/content/custom_tinyvit_config.json'\n",
        "weights_path = '/content/custom_tinyvit_5m_sparse.pth'\n",
        "\n",
        "# 1. Load Config\n",
        "try:\n",
        "    with open(config_path, 'r') as f:\n",
        "        custom_config = json.load(f)\n",
        "    print(\"Loaded custom config.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Config not found at {config_path}\")\n",
        "    custom_config = {}\n",
        "\n",
        "# 2. Prepare Model Arguments\n",
        "# Default parameters for TinyViT-21M\n",
        "model_kwargs = {\n",
        "    'embed_dims': [96, 192, 384, 576],\n",
        "    'depths': [2, 2, 6, 2],\n",
        "    'num_heads': [3, 6, 12, 18],\n",
        "    'window_sizes': [7, 7, 14, 7],  # Standard default\n",
        "    'drop_path_rate': 0.2,\n",
        "    'num_classes': 1000\n",
        "}\n",
        "\n",
        "# Update with custom config values (e.g. window_sizes=[14, 14, 14, 7])\n",
        "# We filter out keys that valid TinyViT.__init__ doesn't accept (like 'sparse_flags' if unsupported)\n",
        "sig = inspect.signature(TinyViT.__init__)\n",
        "valid_keys = set(sig.parameters.keys())\n",
        "\n",
        "for k, v in custom_config.items():\n",
        "    if k in valid_keys:\n",
        "        model_kwargs[k] = v\n",
        "        print(f\"Overriding default '{k}' with custom value: {v}\")\n",
        "    else:\n",
        "        print(f\"Skipping custom config key '{k}' (not supported by TinyViT class)\")\n",
        "\n",
        "# 3. Instantiate Model\n",
        "print(\"Instantiating TinyViT directly with custom arguments...\")\n",
        "try:\n",
        "    model = TinyViT(**model_kwargs)\n",
        "    print(\"Model instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error instantiating model: {e}\")\n",
        "\n",
        "# 4. Load Weights\n",
        "if os.path.exists(weights_path):\n",
        "    try:\n",
        "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
        "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "\n",
        "        # strict=False allows ignoring missing keys (like if sparse args imply extra layers)\n",
        "        msg = model.load_state_dict(state_dict, strict=False)\n",
        "        print(f\"Loaded weights from {weights_path}\")\n",
        "        print(f\"Load result: {msg}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading weights: {e}\")\n",
        "else:\n",
        "    print(f\"Weights file not found at {weights_path}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config not found at /content/custom_tinyvit_config.json\n",
            "Instantiating TinyViT directly with custom arguments...\n",
            "Model instantiated successfully.\n",
            "Weights file not found at /content/custom_tinyvit_5m_sparse.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fac58e6e",
        "outputId": "24b6acec-d7e5-46f8-8643-47ca0d158afd"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "import inspect\n",
        "import sys\n",
        "\n",
        "# Ensure TinyViT is in path\n",
        "if '/content/TinyViT' not in sys.path:\n",
        "    sys.path.append('/content/TinyViT')\n",
        "\n",
        "from models.tiny_vit import TinyViT\n",
        "\n",
        "# Paths\n",
        "config_path = '/content/custom_tinyvit_config.json'\n",
        "weights_path = '/content/custom_tinyvit_5m_sparse.pth'\n",
        "\n",
        "# 1. Load Config\n",
        "try:\n",
        "    with open(config_path, 'r') as f:\n",
        "        custom_config = json.load(f)\n",
        "    print(\"Loaded custom config.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Config not found at {config_path}\")\n",
        "    custom_config = {}\n",
        "\n",
        "# 2. Prepare Model Arguments\n",
        "# Default parameters for TinyViT-21M\n",
        "model_kwargs = {\n",
        "    'embed_dims': [96, 192, 384, 576],\n",
        "    'depths': [2, 2, 6, 2],\n",
        "    'num_heads': [3, 6, 12, 18],\n",
        "    'window_sizes': [7, 7, 14, 7],\n",
        "    'drop_path_rate': 0.2,\n",
        "    'num_classes': 1000\n",
        "}\n",
        "\n",
        "# Update with custom config values\n",
        "sig = inspect.signature(TinyViT.__init__)\n",
        "valid_keys = set(sig.parameters.keys())\n",
        "\n",
        "for k, v in custom_config.items():\n",
        "    if k in valid_keys:\n",
        "        model_kwargs[k] = v\n",
        "        print(f\"Overriding default '{k}' with custom value: {v}\")\n",
        "\n",
        "# 3. Instantiate Model\n",
        "print(\"Instantiating TinyViT directly with custom arguments...\")\n",
        "try:\n",
        "    model = TinyViT(**model_kwargs)\n",
        "    print(\"Model instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error instantiating model: {e}\")\n",
        "\n",
        "# 4. Load Weights\n",
        "if os.path.exists(weights_path):\n",
        "    try:\n",
        "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
        "        state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
        "\n",
        "        msg = model.load_state_dict(state_dict, strict=False)\n",
        "        print(f\"Loaded weights from {weights_path}\")\n",
        "        print(f\"Load result: {msg}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading weights: {e}\")\n",
        "else:\n",
        "    print(f\"Weights file not found at {weights_path}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config not found at /content/custom_tinyvit_config.json\n",
            "Instantiating TinyViT directly with custom arguments...\n",
            "Model instantiated successfully.\n",
            "Weights file not found at /content/custom_tinyvit_5m_sparse.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "adfad9e1",
        "outputId": "db973411-e83e-4d19-8514-4cbb891ad82b"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# 1. Efficiently find the FIRST image in the dataset path without listing everything\n",
        "print(f\"Searching for images in: {path}\")\n",
        "\n",
        "def find_first_image(directory):\n",
        "    extensions = {'.jpg', '.jpeg', '.png'}\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if os.path.splitext(file)[1].lower() in extensions:\n",
        "                return os.path.join(root, file)\n",
        "    return None\n",
        "\n",
        "img_path = find_first_image(path)\n",
        "\n",
        "if not img_path:\n",
        "    print(\"No images found in the dataset path.\")\n",
        "else:\n",
        "    print(f\"Using image: {img_path}\")\n",
        "\n",
        "    # 2. Preprocess the image\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    raw_image = Image.open(img_path).convert('RGB')\n",
        "    input_tensor = transform(raw_image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Display original image\n",
        "    plt.imshow(raw_image)\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2773257006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 1. Efficiently find the FIRST image in the dataset path without listing everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Searching for images in: {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_first_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a42362",
        "outputId": "f00f2bcc-3f48-44f7-ac39-017980e82054"
      },
      "source": [
        "# Container to store qkv outputs\n",
        "qkv_outputs = []\n",
        "\n",
        "# Hook function to capture output of the qkv layer\n",
        "def hook_fn(module, input, output):\n",
        "    # output shape: (Batch, N, 3 * C)\n",
        "    qkv_outputs.append(output.detach().cpu())\n",
        "\n",
        "# Register hook on the last block's qkv layer\n",
        "try:\n",
        "    # Access the attention module of the last block\n",
        "    attn_module = model.layers[-1].blocks[-1].attn\n",
        "    target_layer = attn_module.qkv\n",
        "\n",
        "    # Get num_heads for reshaping later (usually stored in the attn module)\n",
        "    num_heads = getattr(attn_module, 'num_heads', 18) # Default to 18 if not found, based on previous dump\n",
        "\n",
        "    handle = target_layer.register_forward_hook(hook_fn)\n",
        "    print(f\"Hook registered on: {target_layer}\")\n",
        "\n",
        "    # Run inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_tensor)\n",
        "\n",
        "    # Remove hook\n",
        "    handle.remove()\n",
        "\n",
        "    if qkv_outputs:\n",
        "        # Process collected qkv\n",
        "        # Shape: (1, N, 3*C)\n",
        "        qkv = qkv_outputs[0] # (1, 49, 1728)\n",
        "        B, N, C_total = qkv.shape\n",
        "\n",
        "        # Reshape to (B, N, 3, Num_Heads, Head_Dim)\n",
        "        # C_total = 3 * Num_Heads * Head_Dim\n",
        "        head_dim = C_total // (3 * num_heads)\n",
        "\n",
        "        qkv = qkv.reshape(B, N, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
        "        # Shape: (3, B, Num_Heads, N, Head_Dim)\n",
        "\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Calculate Attention: (Q @ K.T) * scale\n",
        "        scale = head_dim ** -0.5\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # Shape: (B, Num_Heads, N, N)\n",
        "\n",
        "        print(f\"Calculated attention shape: {attn.shape}\")\n",
        "\n",
        "        # Average over heads and batch\n",
        "        attn_mean = attn[0].mean(dim=0) # (N, N)\n",
        "\n",
        "        # Visualize\n",
        "        side = int(np.sqrt(attn_mean.shape[0])) # Should be 7\n",
        "        center_idx = (side * side) // 2\n",
        "\n",
        "        attn_map = attn_mean[center_idx, :].reshape(side, side)\n",
        "\n",
        "        # Upsample to image size for visualization\n",
        "        attn_map_resized = cv2.resize(attn_map.numpy(), (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(raw_image)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(raw_image)\n",
        "        plt.imshow(attn_map_resized, cmap='jet', alpha=0.5)\n",
        "        plt.title(\"Attention Map (Last Layer, Center Pixel)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No QKV outputs captured.\")\n",
        "\n",
        "except AttributeError as e:\n",
        "    print(f\"Error accessing layer: {e}\")\n",
        "    print(\"Please verify the model structure.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hook registered on: Linear(in_features=576, out_features=1728, bias=True)\n",
            "An unexpected error occurred: name 'input_tensor' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ebcd763"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"Restarting runtime to apply library changes and fix torch error...\\n\")\n",
        "print(\"After the restart, please re-run your cells starting from the imports.\")\n",
        "\n",
        "# This command kills the current process, causing Colab to automatically restart the kernel\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "\n",
        "# Load TinyViT-5M-224\n",
        "model_og = timm.create_model('tiny_vit_5m_224', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "T6mqYZ-MNlLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15950a98"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure model_og is in eval mode\n",
        "model_og.eval()\n",
        "\n",
        "# Container for qkv outputs\n",
        "og_qkv_outputs = []\n",
        "\n",
        "def og_hook_fn(module, input, output):\n",
        "    og_qkv_outputs.append(output.detach().cpu())\n",
        "\n",
        "# Register Hook on the last block of the last stage\n",
        "# Based on the printed structure: model_og.stages[3].blocks[1].attn.qkv\n",
        "try:\n",
        "    # TinyViT usually has 4 stages (0, 1, 2, 3)\n",
        "    target_layer = model_og.stages[-1].blocks[-1].attn.qkv\n",
        "    # Get num_heads (seen in output as 10 for stage 3)\n",
        "    num_heads = model_og.stages[-1].blocks[-1].attn.num_heads\n",
        "\n",
        "    handle = target_layer.register_forward_hook(og_hook_fn)\n",
        "    print(f\"Hook registered on: {target_layer}\")\n",
        "\n",
        "    # Run Inference\n",
        "    # Ensuring input_tensor is on the same device as model\n",
        "    device = next(model_og.parameters()).device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model_og(input_tensor)\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # Process and Visualize\n",
        "    if og_qkv_outputs:\n",
        "        qkv = og_qkv_outputs[0]\n",
        "        B, N, C_total = qkv.shape\n",
        "\n",
        "        # Reshape: (B, N, 3, Num_Heads, Head_Dim)\n",
        "        head_dim = C_total // (3 * num_heads)\n",
        "        qkv = qkv.reshape(B, N, 3, num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Calculate Attention: (Q @ K.T) * scale\n",
        "        scale = head_dim ** -0.5\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        # Average over heads\n",
        "        attn_mean = attn[0].mean(dim=0)\n",
        "\n",
        "        # Visualize Center Pixel Attention\n",
        "        side = int(np.sqrt(attn_mean.shape[0]))\n",
        "        center_idx = (side * side) // 2\n",
        "        attn_map = attn_mean[center_idx, :].reshape(side, side)\n",
        "\n",
        "        # Resize to image size\n",
        "        attn_map_resized = cv2.resize(attn_map.numpy(), (224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(raw_image)\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(raw_image)\n",
        "        plt.imshow(attn_map_resized, cmap='jet', alpha=0.5)\n",
        "        plt.title(\"Attention Map (model_og)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No QKV outputs captured.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing model_og: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}